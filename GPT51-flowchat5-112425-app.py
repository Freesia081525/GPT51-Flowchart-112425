Below is a complete transformation of your React app into a Streamlit app suitable for a Hugging Face Space, with:

- `agents.yaml` for agent/model configuration and an advanced default prompt  
- Multi-provider support: OpenAI, Gemini, Anthropic, XAI (Grok)  
- API key handling (env first, then optional user input; environment keys never shown)  
- Adjustable system prompt, temperature, max tokens, provider/model before running  
- Interactive visualization (Graphviz flowchart + dashboard)  
- â€œWowâ€ status indicators and an interactive metrics dashboard  

---

## 1. `agents.yaml`

Create a file `agents.yaml` in the project root:

```yaml
# agents.yaml
default_provider: "openai"
default_model: "gpt-4.1-mini"

providers:
  openai:
    name: "OpenAI"
    env_var: "OPENAI_API_KEY"
    models:
      - "gpt-5-nano"
      - "gpt-4.1-mini"
      - "gpt-4o-mini"
  gemini:
    name: "Gemini"
    env_var: "GEMINI_API_KEY"  # or GOOGLE_API_KEY if you prefer
    models:
      - "gemini-2.5-flash"
      - "gemini-2.5-flash-lite"
  anthropic:
    name: "Anthropic"
    env_var: "ANTHROPIC_API_KEY"
    models:
      - "claude-3-5-sonnet-latest"
      - "claude-3-5-haiku-latest"
  xai:
    name: "XAI Grok"
    env_var: "XAI_API_KEY"
    models:
      - "grok-4-fast-reaoning"
      - "grok-3-mini"

agents:
  flowchart_designer:
    description: >
      Converts natural language requirements into a structured flowchart
      JSON specification suitable for visualization.
    provider: "openai"
    model: "gpt-4.1-mini"
    temperature: 0.25
    max_tokens: 2048
    system_prompt: |-
      You are a senior software architect and UX engineer specializing in
      transforming natural-language descriptions of processes into precise,
      machine-readable flowchart specifications.

      ## Primary Goal
      Given:
      - A short description of a process, system, or idea (from the user),
      - A target human language for labels (e.g., English or Chinese),

      You must return a **single valid JSON object** describing a flowchart
      with nodes and edges, and nothing else.

      ## Output Format (Very Important)
      Return **only** a JSON object, no Markdown, no backticks, no commentary.
      The root object MUST have the following shape:

      {
        "title": "Short descriptive title in the target language",
        "description": "1-3 sentence overview in the target language",
        "language": "en" or "zh",
        "nodes": [
          {
            "id": "string-unique-id",
            "label": "Human-readable label in target language",
            "type": "start|end|decision|process|input|output|subprocess|note",
            "group": "Optional phase or swimlane name",
            "lane": "Optional role/responsibility",
            "metadata": {
              "summary": "Optional 1-sentence detail",
              "priority": "low|medium|high",
              "tags": ["optional", "keywords"]
            }
          }
        ],
        "edges": [
          {
            "source": "id-of-source-node",
            "target": "id-of-target-node",
            "label": "Optional edge label in target language",
            "condition": "Optional condition text in target language"
          }
        ]
      }

      Constraints:
      - `id` must be unique across all nodes.
      - Every edge `source` and `target` must reference an existing node `id`.
      - There MUST be exactly one `start` node and at least one `end` node.
      - Use clear, concise labels optimized for visualization.
      - Do not include comments, explanations, or any extra keys beyond the schema above.

      ## Style & Structure Guidelines
      - Organize the flow into logical phases via the `group` property (e.g., "Planning", "Execution").
      - Use `lane` for roles or actors (e.g., "User", "System", "Admin").
      - Use `decision` type for branching points ("Yes/No", "Success/Failure").
      - Favor a moderately granular breakdown (roughly 8-30 nodes, unless the description is extremely small or extremely large).
      - Use `metadata.priority` to indicate relative importance of a node to the overall process.

      ## Robustness & Validation
      - If user requirements are ambiguous, make reasonable assumptions and reflect them in node labels.
      - Ensure the JSON is syntactically valid and parsable.
      - Do NOT wrap JSON in Markdown code fences.
      - Do NOT include any natural-language explanation outside the JSON.

      The target language must match the language requested by the caller (provided separately as "language").
```

You can tweak defaults or model names as needed.

---

## 2. `app.py` (Streamlit for Hugging Face Space)

Below is a single-file Streamlit app that:

- Loads `agents.yaml`  
- Manages provider & model selection  
- Supports OpenAI, Gemini, Anthropic, and XAI Grok  
- Pulls API keys from environment first; if missing, allows secure user input  
- Lets the user edit the system prompt, temperature, and max tokens  
- Builds a flowchart via the selected LLM  
- Renders an interactive dashboard (metrics, status indicators) and a Graphviz flowchart  
- Allows manual JSON editing of the flowchart spec  

```python
# app.py
import os
import time
import json
import re
from typing import Any, Dict, Optional

import streamlit as st
import yaml
from graphviz import Digraph

# --- Optional imports; guard them for deployment flexibility ---
try:
    from openai import OpenAI
except ImportError:
    OpenAI = None

try:
    import google.generativeai as genai
except ImportError:
    genai = None

try:
    from anthropic import Anthropic
except ImportError:
    Anthropic = None

try:
    from xai_sdk import Client as XAIClient
    from xai_sdk.chat import user as xai_user, system as xai_system
except ImportError:
    XAIClient = None
    xai_user = None
    xai_system = None


# ----------------------------
# Utility: Load YAML config
# ----------------------------
@st.cache_resource(show_spinner=False)
def load_agents_config(path: str = "agents.yaml") -> Dict[str, Any]:
    with open(path, "r", encoding="utf-8") as f:
        return yaml.safe_load(f)


# ----------------------------
# Utility: API key management
# ----------------------------
def get_api_key(provider_key: str, provider_cfg: Dict[str, Any]) -> Optional[str]:
    """
    Priority:
    1. Environment variable (never shown)
    2. Session state (user-provided, password field)
    """
    env_var = provider_cfg.get("env_var")
    env_value = os.getenv(env_var or "")

    if "api_keys" not in st.session_state:
        st.session_state.api_keys = {}

    # If env var exists, always use it but never display it.
    if env_value:
        st.session_state.api_keys[provider_key] = "__from_env__"
        return env_value

    # Else fall back to user input in sidebar.
    label = f"{provider_cfg.get('name', provider_key)} API Key"
    user_value = st.sidebar.text_input(label, type="password")
    if user_value:
        st.session_state.api_keys[provider_key] = "__from_user__"
        return user_value

    return None


# ----------------------------
# Utility: Extract JSON from LLM text
# ----------------------------
def extract_json_from_text(text: str) -> Dict[str, Any]:
    """
    Tries to extract a JSON object from the LLM response.
    Prefer ```json ... ``` blocks; fallback to raw text.
    """
    # Look for fenced JSON
    fenced_match = re.search(r"```json\s*(.*?)```", text, re.DOTALL | re.IGNORECASE)
    if fenced_match:
        candidate = fenced_match.group(1).strip()
    else:
        candidate = text.strip()

    # Remove potential leading/trailing junk
    # Try to find first '{' and last '}'
    first = candidate.find("{")
    last = candidate.rfind("}")
    if first != -1 and last != -1 and last > first:
        candidate = candidate[first : last + 1]

    return json.loads(candidate)


# ----------------------------
# LLM backends
# ----------------------------
def call_openai(
    model: str,
    api_key: str,
    system_prompt: str,
    user_prompt: str,
    language: str,
    temperature: float,
    max_tokens: int,
) -> str:
    if OpenAI is None:
        raise RuntimeError("openai package is not installed in this environment.")
    client = OpenAI(api_key=api_key)
    messages = [
        {"role": "system", "content": system_prompt},
        {
            "role": "user",
            "content": f"Language: {language}\n\nUser description:\n{user_prompt}",
        },
    ]
    resp = client.chat.completions.create(
        model=model,
        messages={ "messages": messages } if model.startswith("gpt-5") else messages,  # adjust if needed; keep simple else
        max_tokens=max_tokens,
        temperature=temperature,
    )
    # For latest SDK, response may differ; adjust as needed
    choice = resp.choices[0]
    return choice.message.content if hasattr(choice.message, "content") else choice.message["content"]


def call_gemini(
    model: str,
    api_key: str,
    system_prompt: str,
    user_prompt: str,
    language: str,
    temperature: float,
    max_tokens: int,
) -> str:
    if genai is None:
        raise RuntimeError("google.generativeai package is not installed.")
    genai.configure(api_key=api_key)
    full_prompt = (
        system_prompt
        + "\n\n"
        + f"Language: {language}\n\nUser description:\n{user_prompt}"
    )
    model_obj = genai.GenerativeModel(model)
    resp = model_obj.generate_content(
        full_prompt,
        generation_config={
            "temperature": float(temperature),
            "max_output_tokens": int(max_tokens),
        },
    )
    return resp.text


def call_anthropic(
    model: str,
    api_key: str,
    system_prompt: str,
    user_prompt: str,
    language: str,
    temperature: float,
    max_tokens: int,
) -> str:
    if Anthropic is None:
        raise RuntimeError("anthropic package is not installed.")
    client = Anthropic(api_key=api_key)
    resp = client.messages.create(
        model=model,
        max_tokens=max_tokens,
        temperature=temperature,
        system=system_prompt,
        messages=[
            {
                "role": "user",
                "content": f"Language: {language}\n\nUser description:\n{user_prompt}",
            }
        ],
    )
    # Concatenate all text blocks
    parts = []
    for block in resp.content:
        if getattr(block, "type", None) == "text":
            parts.append(block.text)
        elif isinstance(block, dict) and block.get("type") == "text":
            parts.append(block.get("text", ""))
    return "\n".join(parts)


def call_xai_grok(
    model: str,
    api_key: str,
    system_prompt: str,
    user_prompt: str,
    language: str,
    temperature: float,
    max_tokens: int,
) -> str:
    """
    Sample usage adapted from xAI docs, using xai_sdk.
    """
    if XAIClient is None or xai_user is None or xai_system is None:
        raise RuntimeError("xai_sdk package is not installed.")
    client = XAIClient(api_key=api_key, timeout=3600)

    # Note: Not all xAI models expose temperature/max_tokens in the same way;
    # this is a minimal compatible example.
    chat = client.chat.create(model=model)
    chat.append(xai_system(system_prompt))
    chat.append(
        xai_user(
            f"Language: {language}\n\nUser description:\n{user_prompt}"
        )
    )
    response = chat.sample()
    return response.content


def generate_flowchart_with_llm(
    provider_key: str,
    model: str,
    api_key: str,
    system_prompt: str,
    user_description: str,
    language: str,
    temperature: float,
    max_tokens: int,
) -> Dict[str, Any]:
    """
    Unified entry point for calling the chosen provider/model.
    """
    if provider_key == "openai":
        raw = call_openai(
            model, api_key, system_prompt, user_description, language, temperature, max_tokens
        )
    elif provider_key == "gemini":
        raw = call_gemini(
            model, api_key, system_prompt, user_description, language, temperature, max_tokens
        )
    elif provider_key == "anthropic":
        raw = call_anthropic(
            model, api_key, system_prompt, user_description, language, temperature, max_tokens
        )
    elif provider_key == "xai":
        raw = call_xai_grok(
            model, api_key, system_prompt, user_description, language, temperature, max_tokens
        )
    else:
        raise ValueError(f"Unsupported provider: {provider_key}")

    return extract_json_from_text(raw)


# ----------------------------
# Visualization: Graphviz
# ----------------------------
def flowchart_to_graphviz(spec: Dict[str, Any]) -> Digraph:
    g = Digraph(format="svg")
    g.attr(rankdir="TB", bgcolor="#ffffff00")  # transparent background

    # Basic node style
    g.attr("node", shape="box", style="rounded,filled", fontsize="10")

    nodes = spec.get("nodes", [])
    edges = spec.get("edges", [])

    # Color and shape by type for wow effect
    type_styles = {
        "start": {"shape": "oval", "fillcolor": "#4caf50", "fontcolor": "white"},
        "end": {"shape": "oval", "fillcolor": "#f44336", "fontcolor": "white"},
        "decision": {"shape": "diamond", "fillcolor": "#ff9800", "fontcolor": "black"},
        "process": {"shape": "box", "fillcolor": "#2196f3", "fontcolor": "white"},
        "input": {"shape": "parallelogram", "fillcolor": "#9c27b0", "fontcolor": "white"},
        "output": {"shape": "parallelogram", "fillcolor": "#00bcd4", "fontcolor": "black"},
        "subprocess": {"shape": "box3d", "fillcolor": "#8bc34a", "fontcolor": "black"},
        "note": {"shape": "note", "fillcolor": "#ffeb3b", "fontcolor": "black"},
    }

    for node in nodes:
        nid = str(node.get("id", ""))
        label = node.get("label", nid)
        ntype = node.get("type", "process")
        style = type_styles.get(ntype, type_styles["process"])
        group = node.get("group")
        lane = node.get("lane")
        meta = node.get("metadata", {}) or {}

        caption_parts = [label]
        if lane:
            caption_parts.append(f"[{lane}]")
        if group:
            caption_parts.append(f"({group})")
        if meta.get("priority"):
            caption_parts.append(f"prio: {meta['priority']}")

        caption = "\n".join(caption_parts)

        g.node(
            nid,
            label=caption,
            shape=style["shape"],
            fillcolor=style["fillcolor"],
            fontcolor=style["fontcolor"],
        )

    for edge in edges:
        src = str(edge.get("source", ""))
        tgt = str(edge.get("target", ""))
        label = edge.get("label") or edge.get("condition") or ""
        g.edge(src, tgt, label=label, fontsize="9")

    return g


# ----------------------------
# Streamlit App
# ----------------------------
def main():
    st.set_page_config(
        page_title="AI Flowchart Studio",
        page_icon="ðŸ§©",
        layout="wide",
    )

    cfg = load_agents_config()

    st.title("AI Flowchart Studio")
    st.caption("Multi-provider LLM flowchart generator (OpenAI â€¢ Gemini â€¢ Anthropic â€¢ XAI Grok)")

    # ---------------------------------
    # Sidebar: Controls & Settings
    # ---------------------------------
    st.sidebar.header("LLM & Configuration")

    providers_cfg = cfg.get("providers", {})
    provider_keys = list(providers_cfg.keys())
    default_provider_key = cfg.get("default_provider", provider_keys[0] if provider_keys else "")
    provider_labels = [providers_cfg[k].get("name", k.title()) for k in provider_keys]

    # Select provider
    provider_idx = provider_keys.index(default_provider_key) if default_provider_key in provider_keys else 0
    provider_label = st.sidebar.selectbox("Provider", provider_labels, index=provider_idx)
    selected_provider_key = provider_keys[provider_labels.index(provider_label)]
    provider_cfg = providers_cfg[selected_provider_key]

    # Model selection
    models = provider_cfg.get("models", [])
    default_model = cfg.get("default_model", models[0] if models else "")
    if default_model not in models and models:
        default_model = models[0]
    model = st.sidebar.selectbox("Model", models, index=models.index(default_model) if models else 0)

    # Agent defaults
    agent_cfg = cfg.get("agents", {}).get("flowchart_designer", {})
    default_temp = float(agent_cfg.get("temperature", 0.25))
    default_max_tokens = int(agent_cfg.get("max_tokens", 2048))
    default_system_prompt = agent_cfg.get("system_prompt", "")

    # Advanced generation params
    st.sidebar.subheader("Generation Settings")
    temperature = st.sidebar.slider("Temperature", 0.0, 1.0, default_temp, 0.01)
    max_tokens = st.sidebar.slider("Max Tokens", 256, 4096, default_max_tokens, 64)

    # Language switch for flowchart labels
    language = st.sidebar.radio("Flowchart language", ["en", "zh"], index=0, horizontal=True)

    # Editable system prompt
    with st.sidebar.expander("System Prompt (Advanced)", expanded=False):
        system_prompt = st.text_area(
            "System prompt for the LLM",
            value=default_system_prompt,
            height=300,
        )

    # API key management
    api_key = get_api_key(selected_provider_key, provider_cfg)
    if not api_key:
        st.sidebar.warning(
            "No API key found in environment or input. "
            "Please provide a valid key to run the agent."
        )

    # ---------------------------------
    # Main layout: Input + Results
    # ---------------------------------
    col_left, col_right = st.columns([1, 2], gap="large")

    # ----- Left: Input panel -----
    with col_left:
        st.subheader("Describe your process")
        description = st.text_area(
            "Natural-language description",
            height=240,
            placeholder=(
                "Example:\n"
                "Design a user registration and login flow with email verification, "
                "password reset, and account lockout after 5 failed attempts."
            ),
        )

        generate_button = st.button("Generate Flowchart", type="primary", use_container_width=True)

        # Session state for spec
        if "flowchart_spec" not in st.session_state:
            st.session_state.flowchart_spec = None
        if "last_runtime_ms" not in st.session_state:
            st.session_state.last_runtime_ms = None
        if "last_error" not in st.session_state:
            st.session_state.last_error = None

        if generate_button:
            st.session_state.last_error = None

            if not api_key:
                st.error("Please set a valid API key in the sidebar to continue.")
            elif not description.strip():
                st.error("Please enter a description of the process.")
            else:
                with st.status("Calling LLM and building flowchart...", expanded=True) as status:
                    start_time = time.time()
                    try:
                        status.write(f"Provider: {provider_cfg.get('name')}  |  Model: {model}")
                        spec = generate_flowchart_with_llm(
                            provider_key=selected_provider_key,
                            model=model,
                            api_key=api_key,
                            system_prompt=system_prompt,
                            user_description=description,
                            language=language,
                            temperature=temperature,
                            max_tokens=max_tokens,
                        )
                        st.session_state.flowchart_spec = spec
                        runtime_ms = int((time.time() - start_time) * 1000)
                        st.session_state.last_runtime_ms = runtime_ms
                        status.update(label="Flowchart generated successfully", state="complete")
                    except Exception as e:
                        st.session_state.last_error = str(e)
                        status.update(label="Failed to generate flowchart", state="error")
                        st.error(f"Error: {e}")

        # Mini dashboard & wow indicators
        st.markdown("---")
        st.subheader("Flowchart Stats & Status")

        spec = st.session_state.flowchart_spec
        error_msg = st.session_state.last_error
        runtime_ms = st.session_state.last_runtime_ms

        nodes_count = len(spec.get("nodes", [])) if spec else 0
        edges_count = len(spec.get("edges", [])) if spec else 0

        # Complexity heuristic
        if nodes_count == 0:
            complexity = "N/A"
            complexity_score = 0
        elif nodes_count <= 10:
            complexity = "Low"
            complexity_score = 0.3
        elif nodes_count <= 25:
            complexity = "Medium"
            complexity_score = 0.6
        else:
            complexity = "High"
            complexity_score = 0.9

        # KPI metrics
        m1, m2, m3 = st.columns(3)
        m1.metric("Nodes", nodes_count)
        m2.metric("Edges", edges_count)
        m3.metric("Complexity", complexity)

        st.progress(complexity_score, text=f"Flowchart complexity index: {complexity}")

        # LLM status
        st.markdown("##### LLM Health")
        s1, s2 = st.columns(2)
        if runtime_ms:
            s1.metric("Last runtime (ms)", runtime_ms)
        else:
            s1.metric("Last runtime (ms)", "â€“")
        s2.metric("Provider", provider_cfg.get("name", selected_provider_key))

        if error_msg:
            st.error(f"Last error: {error_msg}")
        elif spec:
            st.success("Ready: Latest flowchart successfully generated.")
        else:
            st.info("Waiting: No flowchart generated yet.")

    # ----- Right: Visualization & JSON editor -----
    with col_right:
        st.subheader("Visualizer & JSON Editor")

        tabs = st.tabs(["Visualizer", "JSON Spec", "JSON Editor"])

        spec = st.session_state.flowchart_spec

        # Tab 1: Visualizer
        with tabs[0]:
            if spec:
                try:
                    graph = flowchart_to_graphviz(spec)
                    st.graphviz_chart(graph)
                except Exception as e:
                    st.error(f"Failed to render graph: {e}")
            else:
                st.info("No flowchart yet. Generate one from the left panel.")

        # Tab 2: JSON Spec (read-only)
        with tabs[1]:
            if spec:
                st.json(spec)
            else:
                st.info("JSON spec will appear here after a flowchart is generated.")

        # Tab 3: JSON Editor (editable)
        with tabs[2]:
            if spec is None:
                st.info("Generate a flowchart first to edit its JSON.")
            else:
                editable_json_str = st.text_area(
                    "Edit flowchart JSON (must remain valid)",
                    value=json.dumps(spec, indent=2, ensure_ascii=False),
                    height=400,
                )
                col_btn1, col_btn2 = st.columns([1, 1])
                with col_btn1:
                    if st.button("Apply JSON Changes"):
                        try:
                            new_spec = json.loads(editable_json_str)
                            # Basic validation
                            if not isinstance(new_spec, dict):
                                raise ValueError("Root must be a JSON object.")
                            if "nodes" not in new_spec or "edges" not in new_spec:
                                raise ValueError("JSON must contain 'nodes' and 'edges' keys.")
                            st.session_state.flowchart_spec = new_spec
                            st.success("JSON updated and applied successfully.")
                        except Exception as e:
                            st.error(f"Invalid JSON: {e}")
                with col_btn2:
                    if st.button("Reset to LLM Output"):
                        # Just re-load from saved original if desired; here we keep current spec
                        st.info("Current spec retained (no stored original to reset to).")

    st.markdown("---")
    st.caption("Powered by OpenAI â€¢ Gemini â€¢ Anthropic â€¢ XAI Grok")


if __name__ == "__main__":
    main()
```

---

## 3. Notes / How It Meets Your Requirements

- **Transform previous design to Streamlit**  
  - Left panel: input, language, generate button, stats â†’ implemented using columns and sidebar controls.  
  - Right panel: tabs for Visualizer / JSON / JSON Editor (replacing React tabs).  
  - Settings (system prompt, model, max tokens, provider) moved into the sidebar.

- **Multi-provider support**  
  - Config via `agents.yaml` (`providers` section).  
  - Runtime selection via `Provider` and `Model` selectboxes in the sidebar.  
  - Includes sample logic for:
    - OpenAI (gpt-5-nano, gpt-4.1-mini, gpt-4o-mini)  
    - Gemini (gemini-2.5-flash, gemini-2.5-flash-lite)  
    - Anthropic (Claude variants)  
    - XAI/Grok (grok-4-fast-reaoning, grok-3-mini) using the `xai_sdk` sample pattern.

- **API keys from environment, optional UI input**  
  - `get_api_key()` checks `env_var` from `agents.yaml`.  
  - If not found, shows a password text input in the sidebar.  
  - Environment keys are never displayed (we mark them internally as `__from_env__`).  
  - User keys are typed into a password field.

- **User-modifiable prompt, max tokens, models**  
  - `System Prompt (Advanced)` expander in the sidebar with editable text_area.  
  - Sliders for temperature and max tokens.  
  - Provider & model selection from `agents.yaml`.

- **Advanced prompt**  
  - Provided in `agents.yaml` under `agents.flowchart_designer.system_prompt` with extensive instructions, JSON schema, style, and robustness guidance.

- **Visualization & Wow Dashboard**  
  - Graphviz chart with node-type-dependent shapes and colors.  
  - Metrics: node count, edge count, complexity level.  
  - Progress bar for complexity index.  
  - LLM health status: last runtime, provider name, success/error indicators.  
  - JSON viewer and full JSON editor tab for advanced users.

---

## 4. 20 Comprehensive Follow-up Questions

1. For the Graphviz-based visualizer, would you like to upgrade to a fully interactive canvas (e.g., pan/zoom, drag nodes) using PyVis or a similar library, or is the current static but styled diagram sufficient?

2. Do you want to support additional node attributes (e.g., estimated duration, cost, risk level) that can be used for richer visual encodings (size, color, border style)?

3. Should the flowchart support swimlanes explicitly (e.g., by grouping/layering nodes by `lane` in the visualization), and if yes, how strict is the layout requirement?

4. Would you like a validation panel that explains detected structural issues in plain language (e.g., â€œ2 decision nodes have only one outgoing edgeâ€) before rendering?

5. Should users be able to save multiple flowcharts in the same session (with names) and switch between them, essentially creating a mini project workspace?

6. Do you want an export feature for the flowchart (e.g., download as PNG/SVG, or JSON spec file) directly from the UI?

7. Would you like the app to support importing an existing JSON flowchart spec from a file upload and then editing/visualizing it?

8. Should the system prompt and agent configuration from `agents.yaml` be editable and persist across sessions (e.g., via a small config editor inside the app), or remain read-only defaults?

9. Would you like to log or visualize token usage and cost estimates per provider/model call (when the APIs expose that information)?

10. Are you planning to orchestrate multiple agents (e.g., one agent for requirements clarification, another for flowchart generation, another for optimization), or is a single generator agent sufficient for now?

11. Should the UI support multiple UI languages (like your original React `uiLanguage` toggle), including localized labels and instructions in the Streamlit interface?

12. Do you want a â€œdiff viewâ€ that highlights changes between the original LLM-generated flowchart JSON and the user-edited version?

13. Would you like preset templates or example prompts (e.g., â€œOnboarding flowâ€, â€œPayment processingâ€, â€œIncident managementâ€) that users can one-click load into the description box?

14. Should the complexity index be made more sophisticated (e.g., based on graph metrics like depth, branching factor, cycles) rather than just node count?

15. Do you want to track and display historical runs in this session (e.g., list of last 5 generations with provider, model, runtime, and small stats summary)?

16. Are there any specific compliance or safety filters you want before rendering the flowchart (e.g., detecting sensitive content and masking it or blocking generation)?

17. Should the app support image inputs in the future (e.g., user uploads a whiteboard sketch and an LLM with vision converts it into a flowchart spec)?

18. Do you want to allow power users to select raw JSON schema variants (e.g., â€œsimple BPMN-likeâ€, â€œdetailed UX flowâ€) which change the system prompt and output shape?

19. How important is reproducibility for youâ€”do you need to show or store seeds and full request parameters so users can reproduce an identical flowchart later?

20. Would you like a lightweight API endpoint (e.g., a simple FastAPI layer or Streamlit `st.experimental_connection`) so external tools can send a description and receive the JSON flowchart spec programmatically?
